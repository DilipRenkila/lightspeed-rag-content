Using taints and tolerations to control logging pod placement

Taints and tolerations allow the node to control which pods should (or should not) be scheduled on them.
Understanding taints and tolerations
A taint allows a node to refuse a pod to be scheduled unless that pod has a matching toleration.

You apply taints to a node through the Node specification (NodeSpec) and apply tolerations to a pod through the Pod specification (PodSpec). When you apply a taint a node, the scheduler cannot place a pod on that node unless the pod can tolerate the taint.

apiVersion: v1
kind: Node
metadata:
  name: my-node
#...
spec:
  taints:
  - effect: NoExecute
    key: key1
    value: value1
#...
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...
Taints and tolerations consist of a key, value, and effect.


If you add a NoSchedule taint to a control plane node, the node must have the node-role.kubernetes.io/master=:NoSchedule taint, which is added by default.
A toleration matches a taint:

If the operator parameter is set to Equal:

If the operator parameter is set to Exists:


The following taints are built into "Red Hat OpenShift Container Platform":

node.kubernetes.io/not-ready: The node is not ready. This corresponds to the node condition Ready=False.

node.kubernetes.io/unreachable: The node is unreachable from the node controller. This corresponds to the node condition Ready=Unknown.

node.kubernetes.io/memory-pressure: The node has memory pressure issues. This corresponds to the node condition MemoryPressure=True.

node.kubernetes.io/disk-pressure: The node has disk pressure issues. This corresponds to the node condition DiskPressure=True.

node.kubernetes.io/network-unavailable: The node network is unavailable.

node.kubernetes.io/unschedulable: The node is unschedulable.

node.cloudprovider.kubernetes.io/uninitialized: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.

node.kubernetes.io/pid-pressure: The node has pid pressure. This corresponds to the node condition PIDPressure=True.
Using tolerations to control log store pod placement
By default, log store pods have the following toleration configurations:

apiVersion: v1
kind: Pod
metadata:
  name: elasticsearch-example
  namespace: openshift-logging
spec:
# ...
  tolerations:
  - effect: NoSchedule
    key: node.kubernetes.io/disk-pressure
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
# ...
apiVersion: v1
kind: Pod
metadata:
  name: lokistack-example
  namespace: openshift-logging
spec:
# ...
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
# ...
You can configure a toleration for log store pods by adding a taint and then modifying the tolerations syntax in the ClusterLogging custom resource (CR).

You have installed the Red Hat OpenShift Logging Operator.

You have installed the OpenShift CLI (oc).

You have deployed an internal log store that is either Elasticsearch or LokiStack.


Add a taint to a node where you want to schedule the logging pods, by running the following command:

Edit the logstore section of the ClusterLogging CR to configure a toleration for the log store pods:


This toleration matches the taint created by the oc adm taint command. A pod with this toleration can be scheduled onto node1.
Using tolerations to control the log visualizer pod placement
You can use a specific key/value pair that is not on other pods to ensure that only the Kibana pod can run on the specified node.

You have installed the Red Hat OpenShift Logging Operator, the OpenShift Elasticsearch Operator, and the OpenShift CLI (oc).


Add a taint to a node where you want to schedule the log visualizer pod by running the following command:

Edit the visualization section of the ClusterLogging CR to configure a toleration for the Kibana pod:


This toleration matches the taint created by the oc adm taint command. A pod with this toleration would be able to schedule onto node1.
Using tolerations to control log collector pod placement
By default, log collector pods have the following tolerations configuration:

apiVersion: v1
kind: Pod
metadata:
  name: collector-example
  namespace: openshift-logging
spec:
# ...
  collection:
    type: vector
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
# ...
You have installed the Red Hat OpenShift Logging Operator and OpenShift CLI (oc).


Add a taint to a node where you want logging collector pods to schedule logging collector pods by running the following command:

Edit the collection stanza of the ClusterLogging custom resource (CR) to configure a toleration for the logging collector pods:


This toleration matches the taint created by the oc adm taint command. A pod with this toleration can be scheduled onto node1.
Additional resources
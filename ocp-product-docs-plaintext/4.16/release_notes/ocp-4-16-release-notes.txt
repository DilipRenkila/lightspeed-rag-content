# Red Hat OpenShift Container Platform 4.16 release notes


Red Hat Red Hat OpenShift Container Platform provides developers and IT organizations with a hybrid cloud application platform for deploying both new and existing applications on secure, scalable resources with minimal configuration and management. Red Hat OpenShift Container Platform supports a wide selection of programming languages and frameworks, such as Java, JavaScript, Python, Ruby, and PHP.
Built on Red Hat Enterprise Linux (RHEL) and Kubernetes, Red Hat OpenShift Container Platform provides a more secure and scalable multitenant operating system for today's enterprise-class applications, while delivering integrated application runtimes and libraries. Red Hat OpenShift Container Platform enables organizations to meet security, privacy, compliance, and governance requirements.

# About this release

Red Hat OpenShift Container Platform (RHSA-2023:7198) is now available. This release uses Kubernetes 1.29 with CRI-O runtime. New features, changes, and known issues that pertain to Red Hat OpenShift Container Platform 4.16 are included in this topic.

Red Hat OpenShift Container Platform 4.16 clusters are available at https://console.redhat.com/openshift. With the Red Hat OpenShift Cluster Manager application for Red Hat OpenShift Container Platform, you can deploy Red Hat OpenShift Container Platform clusters to either on-premises or cloud environments.

Red Hat OpenShift Container Platform 4.16 is supported on Red Hat Enterprise Linux (RHEL) 8.8 and 8.9, and on Red Hat Enterprise Linux CoreOS (RHCOS) 4.15.

You must use RHCOS machines for the control plane, and you can use either RHCOS or RHEL for compute machines.

For Red Hat OpenShift Container Platform 4.12 on x86_64 architecture, Red Hat has added a 6-month Extended Update Support (EUS) phase that extends the total available lifecycle from 18 months to 24 months. For Red Hat OpenShift Container Platform 4.12 running on 64-bit ARM (aarch64), IBM Power&#174; (ppc64le), and IBM Z&#174; (s390x) architectures, the EUS lifecycle remains at 18 months.

Starting with Red Hat OpenShift Container Platform 4.14, each EUS phase for even numbered releases on all supported architectures, including x86_64, 64-bit ARM (aarch64), IBM Power&#174; (ppc64le), and IBM Z&#174; (s390x) architectures, has a total available lifecycle of 24 months.

Starting with Red Hat OpenShift Container Platform 4.14, Red Hat offers a 12-month additional EUS add-on, denoted as Additional EUS Term 2, that extends the total available lifecycle from 24 months to 36 months. The Additional EUS Term 2 is available on all architecture variants of Red Hat OpenShift Container Platform.

For more information about this support, see the Red Hat Red Hat OpenShift Container Platform Life Cycle Policy.

Maintenance support ends for version 4.12 on 25 January 2025 and goes to extended life phase. For more information, see the Red Hat Red Hat OpenShift Container Platform Life Cycle Policy.

Commencing with the 4.16 release, Red Hat is simplifying the administration and management of Red Hat shipped cluster Operators with the introduction of three new life cycle classifications; Platform Aligned, Platform Agnostic, and Rolling Stream. These life cycle classifications provide additional ease and transparency for cluster administrators to understand the life cycle policies of each Operator and form cluster maintenance and upgrade plans with predictable support boundaries. For more information, see OpenShift Operator Life Cycles.

Red Hat OpenShift Container Platform is designed for FIPS. When running Red Hat Enterprise Linux (RHEL) or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, Red Hat OpenShift Container Platform core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.

For more information about the NIST validation program, see Cryptographic Module Validation Program. For the latest NIST status for the individual versions of RHEL cryptographic libraries that have been submitted for validation, see Compliance Activities and Government Standards.

# Red Hat OpenShift Container Platform layered and dependent component support and compatibility

The scope of support for layered and dependent components of Red Hat OpenShift Container Platform changes independently of the Red Hat OpenShift Container Platform version. To determine the current support status and compatibility for an add-on, refer to its release notes. For more information, see the Red Hat Red Hat OpenShift Container Platform Life Cycle Policy.

# New features and enhancements

This release adds improvements related to the following components and concepts.

## Red Hat Enterprise Linux CoreOS (RHCOS)

### RHCOS now uses RHEL 9.4

RHCOS now uses Red Hat Enterprise Linux (RHEL) 9.4 packages in Red Hat OpenShift Container Platform 4.16. These packages ensure that your Red Hat OpenShift Container Platform instances receive the latest fixes, features, enhancements, hardware support, and driver updates. As an Extended Update Support (EUS) release, Red Hat OpenShift Container Platform 4.14 is excluded from this change and will continue to use RHEL 9.2 EUS packages for the entirety of its lifecycle.

### Support for iSCSI boot volumes

With this release, you can now install RHCOS to iSCSI boot devices. Multipathing for iSCSI is also supported. For more information, see Installing RHCOS manually on an iSCSI boot device and Installing RHCOS on an iSCSI boot device using iBFT

## Installation and update

### Cluster API replaces Terraform for AWS installations

In Red Hat OpenShift Container Platform 4.16, the installation program uses Cluster API instead of Terraform to provision cluster infrastructure during installations on AWS. There are several additional required permissions as a result of this change. For more information, see Required AWS permissions for the IAM user.

Additionally, SSH access to control plane and compute machines is no longer open to the machine network, but is restricted to the security groups associated with the control plane and compute plane machines.

### Optional cloud controller manager cluster capability

In Red Hat OpenShift Container Platform 4.16, you can disable the cloud controller manager capability during installation.
For more information, see Cloud controller manager capability.

### Optional additional tags for VMware vSphere

In Red Hat OpenShift Container Platform 4.16, you can add up to ten tags to attach to the VMs provisioned by a VMware vSphere cluster.
These tags are in addition to the unique cluster-specific tag that the installation program uses to identify and remove associated VMs when a cluster is decommissioned.

You can define the tags on the VMware vSphere VMs in the install-config.yaml file during cluster creation.
For more information, see Sample install-config.yaml file for an installer-provisioned VMware vSphere cluster.

You can define tags for compute or control plane machines on an existing cluster by using machine sets.
For more information, see "Adding tags to machines by using machine sets" for compute or control plane machine sets.

### Required administrator acknowledgment when updating from Red Hat OpenShift Container Platform 4.15 to 4.16

Red Hat OpenShift Container Platform 4.16 uses Kubernetes 1.29, which removed several deprecated APIs.

A cluster administrator must provide manual acknowledgment before the cluster can be updated from Red Hat OpenShift Container Platform 4.15 to 4.16. This is to help prevent issues after updating to Red Hat OpenShift Container Platform 4.16, where APIs that have been removed are still in use by workloads, tools, or other components running on or interacting with the cluster. Administrators must evaluate their cluster for any APIs in use that will be removed and migrate the affected components to use the appropriate new API version. After this is done, the administrator can provide the administrator acknowledgment.

All Red Hat OpenShift Container Platform 4.15 clusters require this administrator acknowledgment before they can be updated to Red Hat OpenShift Container Platform 4.16.

For more information, see Preparing to update to Red Hat OpenShift Container Platform 4.16.

### Secure kubeadmin password from being displayed in the console

With this release, you can prevent the kubeadmin password from being displayed in the console after the installation by using the --skip-password-print flag during cluster creation. The password remains accessible in the auth directory.

### OpenShift-based Appliance Builder (Technology Preview)

With this release, the OpenShift-based Appliance Builder is available as a Technology Preview feature.
The Appliance Builder enables self-contained Red Hat OpenShift Container Platform cluster installations, meaning that it does not rely on internet connectivity or external registries.
It is a container-based utility that builds a disk image that includes the Agent-based Installer, which can then be used to install multiple Red Hat OpenShift Container Platform clusters.

For more information, see the OpenShift-based Appliance Builder User Guide.

## Web console

### Administrator perspective

This release introduces the following updates to the Administrator perspective of the web console:

### Developer Perspective

This release introduces the following updates to the Developer perspective of the web console:

## OpenShift CLI (oc)

### Introducing the oc adm upgrade status command (Technology Preview)

Previously, the oc adm upgrade command provided limited information about the status of a cluster update. This release adds the oc adm upgrade status command, which decouples status information from the oc adm upgrade command and provides specific information regarding a cluster update, including the status of the control plane and worker node updates.

### Warning for duplicate resource short names

With this release, if you query a resource by using its short name, the OpenShift CLI  (oc) returns a warning if more than one custom resource definition (CRD) with the same short name exists in the cluster.


```terminal
Warning: short name "ex" could also match lower priority resource examples.test.com
```


### New flag to require confirmation when deleting resources (Technology Preview)

This release introduces a new --interactive flag for the oc delete command. When the --interactive flag is set to true, the resource is deleted only if the user confirms the deletion. This flag is available as a Technology Preview feature.

## IBM Z and IBM LinuxONE



## IBM Power



## Authentication and authorization



## Networking

### OpenShift SDN network plugin blocks future major upgrades

As part of the Red Hat OpenShift Container Platform transition to OVN-Kubernetes as the only supported network plugin, starting with Red Hat OpenShift Container Platform 4.16, if your cluster uses the OpenShift SDN network plugin, you cannot upgrade to future major versions of Red Hat OpenShift Container Platform without migrating to OVN-Kubernetes. For more information about migrating to OVN-Kubernetes, see Migrating from the OpenShift SDN network plugin.

If you attempt an upgrade, the Cluster Network Operator reports the following status:


```yaml
- lastTransitionTime: "2024-04-11T05:54:37Z"
  message: Cluster is configured with OpenShiftSDN, which is not supported in the
    next version. Please follow the documented steps to migrate from OpenShiftSDN
    to OVN-Kubernetes in order to be able to upgrade. https://docs.openshift.com/container-platform/4.16/networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.html
  reason: OpenShiftSDNConfigured
  status: "False"
  type: Upgradeable
```


### Dual-NIC Intel E810 Westport Channel as PTP grandmaster clock (Generally Available)

Configuring linuxptp services as grandmaster clock (T-GM) for dual Intel E810 Westport Channel NICs is now a generally available feature in Red Hat OpenShift Container Platform.
The host system clock is synchronized from the NIC that is connected to the Global Navigation Satellite Systems (GNSS) time source. The second NIC is synced to the 1PPS timing output provided by the NIC that is connected to GNSS.
For more information see Configuring linuxptp services as a grandmaster clock for dual E810 Westport Channel NICs.

### Dual-NIC Intel E810 PTP boundary clock with highly available system clock (Generally Available)

You can configure the linuxptp services ptp4l and phc2sys as a highly available (HA) system clock for dual PTP boundary clocks (T-BC).

For more information, see Configuring linuxptp as a highly available system clock for dual-NIC Intel E810 PTP boundary clocks

### Define multiple CIDR blocks for one network security group (NSG) rule

With this release, IP addresses and ranges are handled more efficiently in NSGs for Red Hat OpenShift Container Platform clusters hosted on Azure. As a result, the maximum limit of CIDRs for all Ingress Controllers in Azure clusters, using the allowedSourceRanges field, increases from approximately 1000 to 4000 CIDRs.

### Migration from OpenShift SDN to OVN-Kubernetes on Nutanix

With this release, migration from the OpenShift SDN network plugin to OVN-Kubernetes is now supported on Nutanix platforms. For more information, see Migration to the OVN-Kubernetes network plugin.

### Improved integration between CoreDNS and egress firewall (Technology Preview)

With this release, OVN-Kubernetes uses a new DNSNameResolver custom resource to keep track of DNS records in your egress firewall rules, and is available as a Technology Preview. This custom resource supports the use of both wildcard DNS names and regular DNS names and allows access to DNS names regardless of the IP addresses associated with its change.

### Parallel node draining during SR-IOV network policy updates

With this update, you can configure the SR-IOV Network Operator to drain nodes in parallel during network policy updates. The option to drain nodes in parallel enables faster rollouts of SR-IOV network configurations. You can use the SriovNetworkPoolConfig custom resource to configure parallel node draining and define the maximum number of nodes in the pool that the Operator can drain in parallel.

For further information, see Configuring parallel node draining during SR-IOV network policy updates.

### SR-IOV Network Operator no longer automatically creates the SriovOperatorConfig CR

As of Red Hat OpenShift Container Platform 4.16, the SR-IOV Network Operator no longer automatically creates a SriovOperatorConfig custom resource (CR). Create the SriovOperatorConfig CR using the procedure described in Configuring the SR-IOV Network Operator.

### Supporting double-tagged packets (QinQ)

This release introduces 802.1Q-in-802.1Q also known as QinQ support. QinQ introduces a second VLAN tag, where the service provider designates the outer tag for their use, offering them flexibility, while the inner tag remains dedicated to the customer’s VLAN. When two VLAN tags are present in a packet, the outer VLAN tag can be either 802.1Q or 802.1ad. The inner VLAN tag must always be 802.1Q.

For more information, see Configuring QinQ support for SR-IOV enabled workloads.

### Configuring a user-managed load balancer for on-premise infrastructure

With this release, you can configure an Red Hat OpenShift Container Platform cluster on any on-premise infrastructure, such as bare-metal, VMware vSphere, Red Hat OpenStack Platform (RHOSP), or Nutanix, to use a user-managed load balancer in place of the default load balancer. For this configuration, you must specify loadBalancer.type: UserManaged in your cluster’s install-config.yaml file.

For more information about this feature on bare-metal infrastructure, see Services for a user-managed load balancer in Setting up the environment for an OpenShift installation.

### Detect and warning for iptables

With this release, if you have pods in your cluster using iptables rules the following event message is given to warn against future deprecation:
This pod appears to have created one or more iptables rules. IPTables is deprecated and will no longer be available in RHEL 10 and later. You should consider migrating to another API such as nftables or eBPF.

For more information, see Getting started with nftables. If you are running third-party software, check with your vendor to ensure they will have an nftables based version available soon.

### Ingress network flows for Red Hat OpenShift Container Platform services

With this release, you can view the ingress network flows for Red Hat OpenShift Container Platform services. You can use this information to manage ingress traffic for your network and improve network security.

For more information, see Red Hat OpenShift Container Platform network flow matrix.

### Patching an existing dual-stack network

With this release, you can add IPv6 virtual IPs (VIPs) for API and Ingress services to an existing dual-stack-configured cluster by patching the cluster infrastructure.

If you have already upgraded your cluster to Red Hat OpenShift Container Platform 4.16 and you need to convert the single-stack cluster network to a dual-stack cluster network, you must specify the following for your cluster in the YAML configuration patch file:

* An IPv4 network for API and Ingress services on the first machineNetwork configuration.
* An IPv6 network for API and Ingress services on the second machineNetwork configuration.

For more information, see Converting to a dual-stack cluster network in Converting to IPv4/IPv6 dual-stack networking.

### Integration of MetalLB and FRR-K8s (Technology Preview)

This release introduces FRR-K8s, a Kubernetes based DaemonSet that exposes a subset of the FRR API in a Kubernetes-compliant manner.
As a cluster administrator, you can use the FRRConfiguration custom resource (CR) to configure the MetalLB Operator to use the FRR-K8s daemon set as the backend.
You can use this to operate FRR services, such as receiving routes.

For more information, see Configuring the integration of MetalLB and FRR-K8s.

## AdminNetworkPolicy is generally available

This feature provides two new APIs, AdminNetworkPolicy (ANP) and BaselineAdminNetworkPolicy (BANP). Before namespaces are created, cluster Administrators can use ANP and BANP to apply cluster-scoped network policies and safeguards for an entire cluster. Because it is cluster scoped, ANP provides Administrators a solution to manage the security of their network at scale without having to duplicate their network policies on each namespace.

For more information, see Converting to a dual-stack cluster network in Converting to IPv4/IPv6 dual-stack networking.

### Live migration to the OVN-Kubernetes network plugin

Previously, when migrating from OpenShift SDN to OVN-Kubernetes, the only available option was an offline migration method. This process included some downtime, during which clusters were unreachable.

This release introduces a live migration method. The live migration method is the process in which the OpenShift SDN network plugin and its network configurations, connections, and associated resources are migrated to the OVN-Kubernetes network plugin without service interruption. It is available for Red Hat OpenShift Container Platform, Red Hat OpenShift Dedicated, Red Hat OpenShift Service on AWS, and Azure Red Hat OpenShift deployment types. It is not available for HyperShift deployment types. This migration method is valuable for deployment types that require constant service availability and offers the following benefits:

* Continuous service availability
* Minimized downtime
* Automatic node rebooting
* Seamless transition from the OpenShift SDN network plugin to the OVN-Kubernetes network plugin

Migration to OVN-Kubernetes is intended to be a one-way process.

For more information, see Live migration to the OVN-Kubernetes network plugin overview.

## Registry



## Storage

### HashiCorp Vault is now available for the Secrets Store CSI Driver Operator (Technology Preview)

You can now use the Secrets Store CSI Driver Operator to mount secrets from HashiCorp Vault to a CSI volume in Red Hat OpenShift Container Platform. The Secrets Store CSI Driver Operator is available as a Technology Preview feature.

For the full list of available secrets store providers, see Secrets store providers.

For information about using the Secrets Store CSI Driver Operator to mount secrets from HashiCorp Vault, see Mounting secrets from HashiCorp Vault.

### Volume cloning supported for Azure File (Technology Preview)

Red Hat OpenShift Container Platform 4.16 introduces volume cloning for the Microsoft Azure File Container Storage Interface (CSI) Driver Operator as a Technology Preview feature. Volume cloning duplicates an existing persistent volume (PV) to help protect against data loss in Red Hat OpenShift Container Platform. You can also use a volume clone just as you would use any standard volume.

For more information, see Azure File CSI Driver Operator and CSI volume cloning.

### Node Expansion Secret is generally available

The Node Expansion Secret feature allows your cluster to expand storage of mounted volumes, even when access to those volumes requires a secret (for example, a credential for accessing a Storage Area Network (SAN) fabric) to perform the node expand operation. Red Hat OpenShift Container Platform 4.16 supports this feature as generally available.

### Changing vSphere CSI maximum number of snapshots is generally available

The default maximum number of snapshots in VMWare vSphere Container Storage Interface (CSI) is 3 per volume. In Red Hat OpenShift Container Platform 4.16, you can now change this maximum number of snapshots to a maximum of 32 per volume. You also have granular control of the maximum number of snapshots for vSAN and Virtual Volume datastores. Red Hat OpenShift Container Platform 4.16 supports this feature as generally available.

For more information, see Changing the maximum number of snapshots for vSphere.

### Persistent volume last phase transition time parameter (Technology Preview)

In Red Hat OpenShift Container Platform 4.16 introduces a new parameter, LastPhaseTransitionTime, which has a timestamp that is updated every time a persistent volume (PV) transitions to a different phase (pv.Status.Phase). This feature is being released with Technology Preview status.

### Persistent storage using CIFS/SMB CSI Driver Operator (Technology Preview)

Red Hat OpenShift Container Platform is capable of provisioning persistent volumes (PVs) with a Container Storage Interface (CSI) driver for the Common Internet File System (CIFS) dialect/Server Message Block (SMB) protocol. The CIFS/SMB CSI Driver Operator that manages this driver is in Technology Preview status.

For more information, see CIFS/SMB CSI Driver Operator.

### RWOP with SELinux context mount is generally available

Red Hat OpenShift Container Platform 4.14 introduced a new access mode with Technical Preview status for persistent volumes (PVs) and persistent volume claims (PVCs) called ReadWriteOncePod (RWOP). RWOP can be used only in a single pod on a single node compared to the existing ReadWriteOnce access mode where a PV or PVC can be used on a single node by many pods. If the driver enables it, RWOP uses the SELinux context mount set in the PodSpec or container, which allows the driver to mount the volume directly with the correct SELinux labels. This eliminates the need to recursively relabel the volume, and pod startup can be significantly faster.

In Red Hat OpenShift Container Platform 4.16, this feature is generally available.

For more information, see Access modes.

### vSphere CSI Driver 3.1 updated CSI topology requirements

To support VMware vSphere Container Storage Interface (CSI) volume provisioning and usage in multi-zonal clusters, the deployment should match certain requirements imposed by CSI driver. These requirements have changed starting with 3.1.0, and although Red Hat OpenShift Container Platform 4.16 accepts both the old and new tagging methods, you should use the new tagging method since VMware considers the old way an invalid configuration; thus to prevent problems, you should not use the old tagging method.

For more information, see vSphere CSI topology requirements.

### Support for configuring thick-provisioned storage

This feature provides support for configuring thick-provisioned storage. If you exclude the deviceClasses.thinPoolConfig field in the LVMCluster custom resource (CR), logical volumes are thick provisioned.
Using thick-provisioned storage includes the following limitations:

* No copy-on-write support for volume cloning.
* No support for VolumeSnapshotClass. Therefore, CSI snapshotting is not supported.
* No support for over-provisioning. As a result, the provisioned capacity of PersistentVolumeClaims (PVCs) is immediately reduced from the volume group.
* No support for thin metrics. Thick-provisioned devices only support volume group metrics.

For information about configuring the LVMCluster CR, see About the LVMCluster custom resource.

### Support for a new warning message when device selector is not configured in the LVMCluster custom resource

This update provides a new warning message when you do not configure the deviceSelector field in the LVMCluster custom resource (CR).

The LVMCluster CR supports a new field, deviceDiscoveryPolicy, which indicates whether the deviceSelector field is configured. If you do not configure the deviceSelector field, LVM Storage automatically sets the deviceDiscoveryPolicy field to RuntimeDynamic. Otherwise, the deviceDiscoveryPolicy field is set to Preconfigured.

It is not recommended to exclude the deviceSelector field from the LMVCluster CR. For more information about the limitations of not configuring the deviceSelector field, see About adding devices to a volume group.

### Support for adding encrypted devices to a volume group

This feature provides support for adding encrypted devices to a volume group. You can enable disk encryption on the cluster nodes during an Red Hat OpenShift Container Platform installation. After encrypting a device, you can specify the path to the LUKS encrypted device in the deviceSelector field in the LVMCluster custom resource. For information about disk encryption, About disk encryption and Configuring disk encryption and mirroring.

For more information about adding devices to a volume group, see About adding devices to a volume group.

## Oracle(R) Cloud Infrastructure



## Operator lifecycle



## Operator development



## Builds



## Machine Config Operator

### Garbage collection of unused rendered machine configs

With this release, you can now garbage collect unused rendered machine configs. By using the oc adm prune renderedmachineconfigs command, you can view the unused rendered machine configs, determine which to remove, then batch delete the rendered machine configs that you no longer need. Having too many machine configs can make working with the machine configs confusing and can also contribute to disk space and performance issues. For more information, see Managing unused rendered machine configs.

### Node disruption policies

By default, when you make certain changes to the parameters in a MachineConfig object, the Machine Config Operator (MCO) drains and reboots the nodes associated with that machine config. However, you can create a node disruption policy in the MCO namespace that defines a set of Ignition config objects changes that would require little or no disruption to your workloads. For more information, see Understanding node restart behaviors after machine config changes.

### On-cluster RHCOS image layering

With Red Hat Enterprise Linux CoreOS (RHCOS) image layering, you can now automatically build the custom layered image directly in your cluster, as a Technology Preview feature. Previously, you needed to build the custom layered image outside of the cluster, then pull the image into the cluster. The image layering feature allows you to extend the functionality of your base RHCOS image by layering additional images onto the base image. For more information, see RHCOS image layering.

## Machine management

### Configuring expanders for the cluster autoscaler

With this release, the cluster autoscaler can use the LeastWaste, Priority, and Random expanders.
You can configure these expanders to influence the selection of machine sets when scaling the cluster.
For more information, see Configuring the cluster autoscaler.

### Managing machines with the Cluster API for VMware vSphere (Technology Preview)

This release introduces the ability to manage machines by using the upstream Cluster API, integrated into Red Hat OpenShift Container Platform, as a Technology Preview for VMware vSphere clusters.
This capability is in addition or an alternative to managing machines with the Machine API.
For more information, see Managing machines with the Cluster API.

## Nodes

### Moving the Vertical Pod Autoscaler Operator pods

The Vertical Pod Autoscaler Operator (VPA) consists of three components: the recommender, updater, and admission controller. The Operator and each component has its own pod in the VPA namespace on the control plane nodes. You can move the VPA Operator and component pods to infrastructure or worker nodes. For more information, see Moving the Vertical Pod Autoscaler Operator components.

### Additional information collected by must-gather

With this release, the oc adm must-gather command collects the following additional information:

* OpenShift CLI (oc) binary version
* Must-gather logs

These additions help identify issues that might stem from using a specific version of oc. The oc adm must-gather command also lists what image was used and if any data could not be gathered in the must-gather logs.

For more information, see About the must-gather tool.

### Upgrading or downgrading host firmware

Beginning with Red Hat OpenShift Container Platform 4.16, you can upgrade or downgrade the firmware for a bare-metal node to a specific version. Metal3 provides the HostFirmwareComponents resource, which describes BIOS and baseboard management controller (BMC) firmware versions. Upgrading or downgrading firmware is useful when deploying an Red Hat OpenShift Container Platform cluster on bare metal with validated patterns that have been tested against specific firmware versions. See About the HostFirmwareComponents resource for additional details.

### Editing the BareMetalHost resource

In Red Hat OpenShift Container Platform 4.16 and later, you can edit the baseboard management controller (BMC) address in the BareMetalHost resource of a bare-metal node. The node must be in the Provisioned, ExternallyProvisioned, Registering, or Available state. Editing the BMC address in the BareMetalHost resource will not result in deprovisioning the node. See Editing a BareMetalHost resource for additional details.

## Monitoring



## Network Observability Operator

The Network Observability Operator releases updates independently from the Red Hat OpenShift Container Platform minor version release stream. Updates are available through a single, Rolling Stream which is supported on all currently supported versions of Red Hat OpenShift Container Platform 4. Information regarding new features, enhancements, and bug fixes for the Network Observability Operator is found in the Network Observability release notes.

## Scalability and performance

### Workload partitioning enhancement

With this release, platform pods deployed with a workload annotation that includes both CPU limits and CPU requests will have the CPU limits accurately calculated and applied as a CPU quota for the specific pod. In prior releases, if a workload partitioned pod had both CPU limits and requests set, they were ignored by the webhook. The pod did not benefit from workload partitioning and was not locked down to specific cores. This update ensures the requests and limits are now interpreted correctly by the webhook.


[NOTE]
----
It is expected that if the values for CPU limits are different from the value for requests in the annotation, the CPU limits are taken as being the same as the requests.
----

For more information, see Workload partitioning.

### Linux Control Groups version 2 is now supported with the performance profile feature

Beginning with Red Hat OpenShift Container Platform 4.16, Control Groups version 2 (cgroup v2), also known as cgroup2 or cgroupsv2, is enabled by default for all new deployments, even when performance profiles are present.

Since Red Hat OpenShift Container Platform 4.14, cgroups v2 has been the default, but the performance profile feature required the use of cgroups v1. This issue has been resolved.

cgroup v1 is still used in upgraded clusters with performance profiles that have initial installation dates before Red Hat OpenShift Container Platform 4.16. cgroup v1 can still be used in the current version by changing the cgroupMode field in the node.config object to v1.

For more information, see Configuring the Linux cgroup version on your nodes.

### Support for increasing the etcd database size (Technology Preview)

With this release, you can increase the disk quota in etcd. This is a Technology Preview feature. For more information, see Increasing the database size for etcd.

### Reserved core frequency tuning

With this release, the Node Tuning Operator supports setting CPU frequencies in the PerformanceProfile for reserved and isolated core CPUs.
This is an optional feature that you can use to define specific frequencies.
The Node Tuning Operator then sets those frequencies by enabling the intel_pstate CPUFreq driver in the Intel hardware.
You must follow Intel&#8217;s recommendations on frequencies for FlexRAN-like applications, which require the default CPU frequency to be set to a lower value than the default running frequency.

### Node Tuning Operator intel_pstate driver default setting

Previously, for the RAN DU-profile, setting the realTime workload hint to true in the PerformanceProfile always disabled the intel_pstate.
With this release, the Node Tuning Operator detects the underlying Intel hardware using TuneD and appropriately sets the intel_pstate kernel parameter based on the processor’s generation.
This decouples the intel_pstate from the realTime and highPowerConsumption workload hints.
The intel_pstate now depends only on the underlying processor generation.

For pre-IceLake processors, the intel_pstate is deactivated by default, whereas for IceLake and later generation processors, the intel_pstate is set to active.

## Edge computing

### Using RHACM PolicyGenerator resources to manage GitOps ZTP cluster policies (Technology Preview)

You can now use PolicyGenerator resources and Red Hat Advanced Cluster Management (RHACM) to deploy polices for managed clusters with GitOps ZTP.
The PolicyGenerator API is part of the Open Cluster Management standard and provides a generic way of patching resources which is not possible with the PolicyGenTemplate API.
Using PolicyGenTemplate resources to manage and deploy polices will be deprecated in an upcoming OpenShift Container Platform release.

For more information, see Configuring managed cluster policies by using PolicyGenerator resources.


[NOTE]
----
The PolicyGenerator API does not currently support merging patches with custom Kubernetes resources that contain lists of items. For example, in PtpConfig CRs.
----

### TALM policy remediation

With this release, Topology Aware Lifecycle Manager (TALM) uses a Red Hat Advanced Cluster Management (RHACM) feature to remediate inform policies on managed clusters. This enhancement removes the need for the Operator to create enforce copies of inform policies during policy remediation. This enhancement also reduces the workload on the hub cluster due to copied policies, and can reduce the overall time required to remediate policies on managed clusters.

For more information, see Update policies on managed clusters.

### Accelerated provisioning of GitOps ZTP (Technology Preview)

With this release, you can reduce the time taken for cluster installation by using accelerated provisioning of GitOps ZTP for single-node OpenShift.
Accelerated ZTP speeds up installation by applying Day 2 manifests derived from policies at an earlier stage.

The benefits of accelerated provisioning of GitOps ZTP increase with the scale of your deployment.
Full acceleration gives more benefit on a larger number of clusters.
With a smaller number of clusters, the reduction in installation time is less significant.

For more information, see Accelerated provisioning of GitOps ZTP.

## Hosted control planes



## Insights Operator



# Notable technical changes

Red Hat OpenShift Container Platform 4.16 introduces the following notable technical changes.

## HAProxy version 2.8

Red Hat OpenShift Container Platform 4.16 uses HAProxy 2.8.

## SHA-1 certificates no longer supported for use with HAProxy

SHA-1 certificates are no longer supported for use with HAProxy. Both existing and new routes using SHA-1 certificates in Red Hat OpenShift Container Platform 4.16 are rejected and no longer function. For more information about creating secure routes, see Secured Routes.

## etcd tuning parameters

With this release, the etcd tuning parameters can be set to values that optimize performance and decrease latency, as follows.

* "" (Default)
* Standard
* Slower

## RHCOS dasd image artifacts no longer supported on IBM Z(R) and IBM(R) LinuxONE (s390x)

With this release, dasd image artifacts for the s390x architecture are removed from the Red Hat OpenShift Container Platform image building pipeline. You can still use the metal4k image artifact, which is identical and contains the same functionality.

## Support for EgressIP with ExternalTrafficPolicy=Local services

Previously, it was unsupported for EgressIP selected pods to also serve as backends for services with externalTrafficPolicy set to Local. When attempting this configuration, service ingress traffic reaching the pods was incorrectly rerouted to the egress node hosting the EgressIP. This affected how responses to incoming service traffic connections were handled and led to non-functional services when externalTrafficPolicy was set to Local, as connections were dropped and the service became unavailable.

With Red Hat OpenShift Container Platform 4.16, OVN-Kubernetes now supports the use of ExternalTrafficPolicy=Local services and EgressIP configurations at the same time on the same set of selected pods. OVN-Kubernetes now only reroutes the traffic originating from the EgressIP pods towards the egress node while routing the responses to ingress service traffic from the EgressIP pods via the same node where the pod is located.

## Operator API renamed to ClusterExtension

Earlier Technology Preview phases of Operator Lifecycle Manager (OLM) 1.0 introduced a new Operator API, provided as operator.operators.operatorframework.io by the Operator Controller component. In Red Hat OpenShift Container Platform 4.16, this API is renamed ClusterExtension, provided as clusterextension.olm.operatorframework.io, for this Technology Preview phase of OLM 1.0.

This API still streamlines management of installed extensions, which includes Operators via the registry+v1 bundle format, by consolidating user-facing APIs into a single object. The rename to ClusterExtension addresses the following:

* More accurately reflects the simplified functionality of extending a cluster's capabilities
* Better represents a more flexible packaging format
* Cluster prefix clearly indicates that ClusterExtension objects are cluster-scoped, a change from legacy OLM where Operators could be either namespace-scoped or cluster-scoped

## Legacy service account API token secrets are no longer generated for each service account

Prior to Red Hat OpenShift Container Platform 4.16, when the integrated OpenShift image registry was enabled, a legacy service account API token secret was generated for every service account in the cluster. Starting with Red Hat OpenShift Container Platform 4.16, when the integrated OpenShift image registry is enabled, the legacy service account API token secret is no longer generated for each service account.

Additionally, when the integrated OpenShift image registry is enabled, the image pull secret generated for every service account no longer uses a legacy service account API token. Instead, the image pull secret now uses a bound service account token that is automatically refreshed before it expires.

For more information, see Automatically generated image pull secrets.

For information about detecting legacy service account API token secrets that are in use in your cluster or deleting them if they are not needed, see the Red Hat Knowledgebase article Long-lived service account API tokens in OpenShift Container Platform.

## Support for external cloud authentication providers

In this release, the functionality to authenticate to private registries on Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure clusters is moved from the in-tree provider to binaries that ship with Red Hat OpenShift Container Platform.
This change supports the default external cloud authentication provider behavior that is introduced in Kubernetes 1.29.

# Deprecated and removed features

Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in Red Hat OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. For the most recent list of major functionality deprecated and removed within Red Hat OpenShift Container Platform 4.16, refer to the table below. Additional details for more functionality that has been deprecated and removed are listed after the table.

In the following tables, features are marked with the following statuses:

* General Availability
* Deprecated
* Removed

## Operator lifecycle and development deprecated and removed features



## Images deprecated and removed features



## Monitoring deprecated and removed features



## Installation deprecated and removed features



1. While the OpenShift SDN network plugin is no longer supported by the installation program in version 4.15, you can upgrade a cluster that uses the OpenShift SDN plugin from version 4.14 to version 4.15.

## Updating clusters deprecated and removed features



## Storage deprecated and removed features



## Specialized hardware and driver enablement deprecated and removed features



## Networking deprecated and removed features



## Web console deprecated and removed features



## Node deprecated and removed features



## OpenShift CLI (oc) deprecated and removed features



## Workloads deprecated and removed features



## Bare metal monitoring deprecated and removed features



## Deprecated features

### Linux Control Groups version 1 is now deprecated

In Red Hat Enterprise Linux (RHEL) 9, the default mode is cgroup v2. When Red Hat Enterprise Linux (RHEL) 10 is released, systemd will not support booting in the cgroup v1 mode and only cgroup v2 mode will be available. As such, cgroup v1 is deprecated in Red Hat OpenShift Container Platform 4.16 and later. cgroup v1 will be removed in a future Red Hat OpenShift Container Platform release.

### Cluster Samples Operator

The Cluster Samples Operator is deprecated with the Red Hat OpenShift Container Platform 4.16 release. The Cluster Samples Operator will stop managing and providing support to the non-S2I samples (image streams and templates). No new templates, samples or non-Source-to-Image (Non-S2I) image streams will be added to the Cluster Samples Operator. However, the existing S2I builder image streams and templates will continue to receive updates until the Cluster Samples Operator is removed in a future release.

### Package-based RHEL compute machines

With this release, installation of package-based RHEL worker nodes is deprecated. In a subsequent future release, RHEL worker nodes will be removed and no longer supported.

RHCOS image layering will replace this feature and supports installing additional packages on the base operating system of your worker nodes.

For more information about image layering, see RHCOS image layering.

## Removed features

### Deprecated disk partition configuration method

The nodes.diskPartition section in the SiteConfig custom resource (CR) is deprecated with the Red Hat OpenShift Container Platform 4.16 release. It has been replaced with the ignitionConfigOverride method, which provides a more flexible way of creating a disk partition for any use case.

For more information, see Configuring disk partitioning with SiteConfig.

### Removal of platform Operators and plain bundles (Technology Preview)

Red Hat OpenShift Container Platform 4.16 removes platform Operators (Technology Preview) and plain bundles (Technology Preview), which were protoypes for Operator Lifecycle Manager (OLM) 1.0 (Technology Preview).

### Dell iDRAC driver for BMC addressing removed

Red Hat OpenShift Container Platform 4.16 supports baseboard management controller (BMC) addressing with Dell servers as documented in BMC addressing for Dell iDRAC. Specifically, it supports idrac-virtualmedia, redfish, and ipmi. In previous versions, idrac was included, but not documented or supported. In Red Hat OpenShift Container Platform 4.16, idrac has been removed.

### MetalLB AddressPool custom resource definition (CRD) removed

The MetalLB AddressPool custom resource definition (CRD) has been deprecated for several versions. However, in this release, the CRD is completely removed. The sole supported method of configuring MetalLB address pools is by using the IPAddressPools CRD.

### Service Binding Operator documentation removed

With this release, the documentation for the Service Binding Operator (SBO) has been removed because this Operator is no longer supported.

### AliCloud CSI Driver Operator is no longer supported

Red Hat OpenShift Container Platform 4.16 no longer supports AliCloud Container Storage Interface (CSI) Driver Operator.

### Beta APIs removed from Kubernetes 1.29

Kubernetes 1.29 removed the following deprecated APIs, so you must migrate manifests and API clients to use the appropriate API version. For more information about migrating removed APIs, see the Kubernetes documentation.



## Notice of future deprecation



# Bug fixes

## API Server and Authentication

## Bare Metal Hardware Provisioning

## Builds

## Cloud Compute

## Cloud Credential Operator

* Previously, the Cloud Credential Operator was missing some permissions required to create a private cluster on Microsoft Azure.
These missing permissions prevented installation of an Azure private cluster using Microsoft Entra Workload ID.
This release includes the missing permissions and enables installation of an Azure private cluster using Workload ID.
(OCPBUGS-25193)
* Previously, a bug caused the Cloud Credential Operator (CCO) to report an incorrect mode in the metrics. Even though the cluster was in the default mode, the metrics reported that it was in the credentials removed mode. This update uses a live client in place of a cached client so that it is able to obtain the root credentials, and the CCO no longer reports an incorrect mode in the metrics. (OCPBUGS-26488)
* Previously, the Cloud Credential Operator credentials mode metric on an Red Hat OpenShift Container Platform cluster that uses Microsoft Entra Workload ID reported using manual mode.
With this release, clusters that use Workload ID are updated to report that they are using manual mode with pod identity.
(OCPBUGS-27446)
* Previously, creating an Amazon Web Services (AWS) root secret on a bare metal cluster caused the Cloud Credential Operator (CCO) pod to crash.
The issue is resolved in this release.
(OCPBUGS-28535)
* Previously, removing the root credential from a Google Cloud Platform (GCP) cluster that used the Cloud Credential Operator (CCO) in mint mode caused the CCO to become degraded after approximately one hour.
In a degraded state, the CCO cannot manage the component credential secrets on a cluster.
The issue is resolved in this release.
(OCPBUGS-28787)
* Previously, the Cloud Credential Operator (CCO) checked for a nonexistent s3:HeadBucket permission during installation on Amazon Web Services (AWS).
When the CCO failed to find this permission, it considered the provided credentials insufficient for mint mode.
With this release, the CCO no longer checks for the nonexistent permission.
(OCPBUGS-31678)

## Cluster Version Operator

## Developer Console

## etcd Cluster Operator

## Hosted Control Plane

## Image Registry

## Installer

## Kubernetes Controller Manager

## Kubernetes Scheduler

## Machine Config Operator

## Management Console

## Monitoring

## Networking

* Previously, the API documentation for the status.componentRoutes.currentHostnames field in the Ingress API included developer notes. After you entered the oc explain ingresses.status.componentRoutes.currentHostnames --api-version=config.openshift.io/v1 command, developer notes would show in the output along with the intended information. With this release, the developer notes are removed from the status.componentRoutes.currentHostnames field, so that after you enter the command, the output lists current hostnames used by the route. (OCPBUGS-31058)
* Previously, the load balancing algorithm did not differentiate between active and inactive services when determining weights, and it employed a random algorithm excessively in environments with many inactive services or environments routing backends with weight 0. This led to increased memory usage and a higher risk of excessive memory consumption. With this release, changes optimize traffic direction towards active services only and prevent unnecessary use of a random algorithm with higher weights, reducing the potential for excessive memory consumption. (OCPBUGS-29690)
* Previously, if multiple routes were specified in the same certificate or if a route specified the default certificate as a custom certificate, and HTTP/2 was enabled on the router, an HTTP/2 client could perform connection coalescing on routes. Clients, such as a web browser, could re-use connections and potentially connect to the wrong backend server. With this release, the Red Hat OpenShift Container Platform router now checks when the same certificate is specified on more than one route or when a route specifies the default certificate as a custom certificate. When either one of these conditions is detected, the router configures the HAProxy load balancer so to not allow HTTP/2 client connections to any routes that use these certificate.(OCPBUGS-29373)
* Previously, if you configured a deployment with the routingViaHost parameter set to true, traffic failed to reach the IPv6 ExternalTrafficPolicy=Local load balancer service. With this release, the issue is fixed. (OCPBUGS-27211)
* Previously, a pod selected by an EgressIp object that was hosted on a secondary network interface controller (NIC) caused connections to node IP addresses to timeout. With this release, the issue is fixed. (OCPBUGS-26979)
* Previously, a leap file package that the Red Hat OpenShift Container Platform Precision Time Protocol (PTP) Operator installed could not be used by the ts2phc process because the package expired. With this release, the leap file package is updated to read leap events from Global Positioning System (GPS) signals and update the offset dynamically so that the expired package situation no longer occurs. (OCPBUGS-25939)
* Previously, pods assigned an IP from the pool created by the Whereabouts CNI plugin were getting stuck in the ContainerCreating state after a node forced a reboot. With this release, the Whereabouts CNI plugin issue associated with the IP allocation after a node force reboot is resolved. (OCPBUGS-24608)
* Previously, there was a conflict between two scripts on Red Hat OpenShift Container Platform in IPv6, including single and dual-stack, deployments. One script set the hostname to a fully qualified domain name (FQDN) but the other script might set it to a short name too early. This conflict happened because the event that triggered setting the hostname to FQDN might run after the script that set it to a short name. This occurred due to asynchronous network events. With this release, new code has been added to ensure that the FQDN is set properly. This new code ensures that there is a wait for a specific network event before allowing the hostname to be set. (OCPBUGS-22324)
* Previously, if a pod selected by an EgressIP through a secondary interface had its label removed, another pod in the same namespace would also lose its EgressIP assignment, breaking its connection to the external host. With this release, the issue is fixed, so that when a pod label is removed and it stops using the EgressIP, other pods with the matching label continue to use the EgressIP without interruption. (OCPBUGS-20220)
* Previously, EgressIP pods hosted by a secondary interface would not failover because of a race condition. Users would receive an error message indicating that the EgressIP pod could not be assigned because it conflicted with an existing IP address. With this update, the EgressIP pod moves to an egress node. (OCPBUGS-20209)
* Previously, when a MAC address changed on the physical interface being used by OVN-Kubernetes, it would not be updated correctly within OVN-Kubernetes and could cause traffic disruption and Kube API outages from the node for a prolonged period of time. This was most common when a bond interface was being used, where the MAC address of the bond might swap depending on which device was the first to come up. With this release, the issues if fixed so that OVN-Kubernetes dynamically detects MAC address changes and updates it correctly. (OCPBUGS-18716)
* Previously, the global navigation satellite system (GNSS) module was capable of reporting both the GPS fix position and the GNSS offset position, which represents the offset between the GNSS module and the constellations. The previous T-GM did not use the ubloxtool CLI tool to probe the ublox module for reading offset and fix positions. Instead, it could only read the GPS fix information via GPSD. The reason for this was that the previous implementation of the ubloxtool CLI tool took two seconds to receive a response, and with every call it increased CPU usage by threefold. With this update, the ubloxtool request is now optimized, and the GPS offset position is now available. (OCPBUGS-17422)

* Previously, IPv6 was unsupported when assigning an egress IP to a network interface that was not the primary network interface. This issue has been resolved, and the egress IP can be IPv6. (OCPBUGS-24271)

## Node

## Node Tuning Operator (NTO)

## OpenShift CLI (oc)

## Operator Lifecycle Manager (OLM)

* Previously, Operator catalogs were not being refreshed properly, due to the imagePullPolicy field being set to IfNotPresent for the index image. This bug fix updates OLM to use the appropriate image pull policy for catalogs, and as a result catalogs are refreshed properly. (OCPBUGS-30132)
* Previously, cluster upgrades could be blocked due to OLM getting stuck in a CrashLoopBackOff state. This was due to an issue with resources having multiple owner references. This bug fix updates OLM to avoid duplicate owner references and only validate the related resources that it owns. As a result, cluster upgrades can proceed as expected. (OCPBUGS-28744)
* Previously, default OLM catalog pods backed by a CatalogSource object would not survive an outage of the node that they were being run on. The pods remained in termination state, despite the tolerations that should move them. This caused Operators to no longer be able to be installed or updated from related catalogs. This bug fix updates OLM so catalog pods that get stuck in this state are deleted. As a result, catalog pods now correctly recover from planned or unplanned node maintenance. (OCPBUGS-32183)
* Previously, installing an Operator could sometimes fail if the same Operator had been previously installed and uninstalled. This was due to a caching issue. This bug fix updates OLM to correctly install the Operator in this scenario, and as a result this issue no longer occurs. (OCPBUGS-31073)
* Previously, the catalogd component could crash loop after an etcd restore. This was due to the garbage collection process causing a looping failure state when the API server was unreachable. This bug fix updates catalogd to add a retry loop, and as a result catalogd no longer crashes in this scenario. (OCPBUGS-29453)
* Previously, the default catalog source pod would not receive updates, requiring users to manually recreate it to get updates. This was caused by image IDs for catalog pods not getting detected correctly. This bug fix updates OLM to correctly detect catalog pod image IDs, and as a result, default catalog sources are updated as expected. (OCPBUGS-31438)
* Previously, users could experience Operator installation errors due to OLM not being able to find existing ClusterRoleBinding or Service resources and creating them a second time. This bug fix updates OLM to pre-create these objects, and as a result these installation errors no longer occur. (OCPBUGS-24009)

## OpenShift API server

## Red Hat Enterprise Linux CoreOS (RHCOS)

## Scalability and performance

## Storage

* Previously, some LVMVolumeGroupNodeStatus operands were not deleted on the cluster during the deletion of the LVMCluster custom resource (CR). With this release, deleting the LVMCluster CR triggers the deletion of all the LVMVolumeGroupNodeStatus operands. (OCPBUGS-32954)
* Previously, LVM Storage uninstallation was stuck waiting for the deletion of the LVMVolumeGroupNodeStatus operands. This fix corrects the behavior by ensuring all operands are deleted, allowing LVM Storage to be uninstalled without delay. (OCPBUGS-32753)
* Previously, LVM Storage did not support minimum storage size for persistent volume claims (PVCs). This can lead to mount failures while provisioning PVCs. With this release, LVM Storage supports minimum storage size for PVCs. The following are the minimum storage sizes that you can request for each file system type:
* block: 8 MiB
* xfs: 300 MiB
* ext4: 32 MiB

If the value of the requests.storage field in the PersistentVolumeClaim object is less than the minimum storage size, the requested storage size is rounded to the minimum storage size. If the value of the limits.storage field is less than the minimum storage size, PVC creation fails with an error. (OCPBUGS-30266)
* Previously, LVM Storage created persistent volume claims (PVCs) with storage size requests that were not multiples of the disk sector size. This can cause issues during LVM2 volume creation. This fix corrects the behavior by rounding the storage size requested by PVCs to the nearest multiple of 512. (OCPBUGS-30032)
* Previously, the LVMCluster custom resource (CR) contained an excluded status element for a device that is set up correctly. This fix filters the correctly set device from being considered for an excluded status element, so it only appears in the ready devices. (OCPBUGS-29188)

## Windows containers

# Technology Preview features status

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

Technology Preview Features Support Scope

In the following tables, features are marked with the following statuses:

* Technology Preview
* General Availability
* Not Available
* Deprecated

## Networking Technology Preview features



## Storage Technology Preview features



## Installation Technology Preview features



## Node Technology Preview features



## Multi-Architecture Technology Preview features



## Specialized hardware and driver enablement Technology Preview features



## Web console Technology Preview features



## Scalability and performance Technology Preview features



## Operator lifecycle and development Technology Preview features



## Monitoring Technology Preview features



## Red Hat OpenStack Platform (RHOSP) Technology Preview features



## Hosted Control Plane Technology Preview features



## Machine management Technology Preview features



## Authentication and authorization Technology Preview features



## Machine Config Operator Technology Preview features



## Edge computing Technology Preview features



# Known issues

* The oc annotate command does not work for LDAP group names that contain an equal sign (=), because the command uses the equal sign as a delimiter between the annotation name and value. As a workaround, use oc patch or oc edit to add the annotation. (BZ#1917280)
* Run Once Duration Override Operator (RODOO) cannot be installed on clusters managed by the Hypershift Operator. (OCPBUGS-17533)
* When you run Cloud-native Network Functions (CNF) latency tests on an Red Hat OpenShift Container Platform cluster, the oslat test can sometimes return results greater than 20 microseconds. This results in an oslat test failure.
(RHEL-9279)
* For a bonding network interface that holds a br-ex bridge device, do not set the mode=6 balance-alb bond mode in a node network configuration. This bond mode is not supported on Red Hat OpenShift Container Platform and it can cause the Open vSwitch (OVS) bridge device to disconnect from your networking environment. (OCPBUGS-34430).

* Do not update firmware for the BareMetalHosts (BMH) resource by editing the HostFirmwareComponents resource. Otherwise, the BMH remains in the Preparing state and executes the firmware update repeatedly. There is no workaround. (OCPBUGS-35559)

* The current PTP grandmaster clock (T-GM) implementation has a single National Marine Electronics Association (NMEA) sentence generator sourced from the GNSS without a backup NMEA sentence generator.
If NMEA sentences are lost before reaching the e810 NIC, the T-GM cannot synchronize the devices in the network synchronization chain and the PTP Operator reports an error.
A proposed fix is to report a FREERUN event when the NMEA string is lost.
Until this limitation is addressed, T-GM does not support PTP clock holdover state.
(OCPBUGS-19838)

* When a worker node's Topology Manager policy is changed, the NUMA-aware secondary pod scheduler does not respect this change, which can result in incorrect scheduling decisions and unexpected topology affinity errors. As a workaround, restart the NUMA-aware scheduler by deleting the NUMA-aware scheduler pod. (OCPBUGS-34583)
* Due to an issue with Kubernetes, the CPU Manager is unable to return CPU resources from the last pod admitted to a node to the pool of available CPU resources. These resources are allocatable if a subsequent pod is admitted to the node. However, this pod then becomes the last pod, and again, the CPU manager cannot return this pod's resources to the available pool.

This issue affects CPU load balancing features, which depend on the CPU Manager releasing CPUs to the available pool. Consequently, non-guaranteed pods might run with a reduced number of CPUs. As a workaround, schedule a pod with a best-effort CPU Manager policy on the affected node. This pod will be the last admitted pod and this ensures the resources will be correctly released to the available pool.(OCPBUGS-17792)
* After applying a SriovNetworkNodePolicy resource, the CA certificate might be replaced during SR-IOV Network Operator webhook reconciliation. As a consequence, you might see unknown authority errors when applying SR-IOV Network node policies. As a workaround, try to re-apply the failed policies. (OCPBUGS-32139)
* If you delete a SriovNetworkNodePolicy resource for a virtual function with a vfio-pci driver type, the SR-IOV Network Operator is unable to reconcile the policy. As a consequence the sriov-device-plugin pod enters a continuous restart loop. As a workaround, delete all remaining policies affecting the physical function, then re-create them. (OCPBUGS-34934)

# Asynchronous errata updates

Security, bug fix, and enhancement updates for Red Hat OpenShift Container Platform 4.16 are released as asynchronous errata through the Red Hat Network. All Red Hat OpenShift Container Platform 4.16 errata is available on the Red Hat Customer Portal. See the Red Hat OpenShift Container Platform Life Cycle for more information about asynchronous errata.

Red Hat Customer Portal users can enable errata notifications in the account settings for Red Hat Subscription Management (RHSM). When errata notifications are enabled, users are notified through email whenever new errata relevant to their registered systems are released.


[NOTE]
----
Red Hat Customer Portal user accounts must have systems registered and consuming Red Hat OpenShift Container Platform entitlements for Red Hat OpenShift Container Platform errata notification emails to generate.
----

This section will continue to be updated over time to provide notes on enhancements and bug fixes for future asynchronous errata releases of Red Hat OpenShift Container Platform 4.16. Versioned asynchronous releases, for example with the form Red Hat OpenShift Container Platform 4.16.z, will be detailed in subsections. In addition, releases in which the errata text cannot fit in the space provided by the advisory will be detailed in subsections that follow.


[IMPORTANT]
----
For any Red Hat OpenShift Container Platform release, always review the instructions on updating your cluster properly.
----

## RHSA-2024:XXXX - Red Hat OpenShift Container Platform 4.16.0 image release, bug fix, and security update advisory

Issued: TBD

Red Hat OpenShift Container Platform release 4.16.0, which includes security updates, is now available. The list of bug fixes that are included in the update is documented in the RHSA-2023:7198 advisory. The RPM packages that are included in the update are provided by the RHSA-2023:7201 advisory.

Space precluded documenting all of the container images for this release in the advisory.

You can view the container images in this release by running the following command:


```terminal
$ oc adm release info 4.16.0 --pullspecs
```
